[#node-health-issues]
== Node health issues

The following tables describe node health issues that can be detected by the node monitoring agent. There are two types of issues:

* Condition – A terminal issue that warrants a remediation action like an instance replacement or reboot. When auto repair is enabled, Amazon EKS will do a repair action, either as a node replacement or reboot. For more information, see <<status-node-conditions>>.

* Event – A temporary issue or sub-optimal node configuration. No auto repair action will take place. For more information, see <<status-node-events>>.

[#node-health-AcceleratedHardware]
=== AcceleratedHardware node health issues

The monitoring condition is `AcceleratedHardwareReady` for issues in the following table that have a severity of "Condition".

If auto repair is enabled, the repair actions that are listed start 10 minutes after the issue is detected. For more information on XID errors, see link:https://docs.nvidia.com/deploy/xid-errors/index.html#topic_5_1[Xid Errors] in the _NVIDIA GPU Deployment and Management Documentation_. For more information on the individual XID messages, see link:https://docs.nvidia.com/deploy/gpu-debug-guidelines/index.html#understanding-xid-messages[Understanding Xid Messages] in the _NVIDIA GPU Deployment and Management Documentation_.

[%header,cols="3"]
|===

|Name
|Severity
|Description

|DCGMDiagnosticFailure
|Condition
|A test case from the DCGM active diagnostics test suite failed.

|DCGMError
|Condition
|Connection to the DCGM host process was lost or could not be established.

|DCGMFieldError[Code]
|Event
|DCGM detected GPU degredation through a field identifier.

|DCGMHealthCode[Code]
|Event
|A DCGM health check failed in a non-fatal manner.

|DCGMHealthCode[Code]
|Condition
|A DCGM health check failed in a fatal manner.

|NeuronDMAError
|Condition
|A DMA engine encountered an unrecoverable error.

|NeuronHBMUncorrectableError
|Condition
|An HBM encountered an uncorrectable error and produced incorrect results.

|NeuronNCUncorrectableError
|Condition
|A Neuron Core uncorrectable memory error was detected.

|NeuronSRAMUncorrectableError
|Condition
|An on-chip SRAM encountered a parity error and produced incorrect results.

|NvidiaDeviceCountMismatch
|Event
|The number of GPUs visible through NVML is inconsistent with the NVIDIA device count on the filesystem.

|NvidiaDoubleBitError
|Condition
|A double bit error was produced by the GPU driver.

|NvidiaNCCLError
|Event
|A segfault occurred in the NVIDIA Collective Communications library (`libnccl`).

|NvidiaNVLinkError
|Condition
|NVLink errors were reported by the GPU driver.

|NvidiaPCIeError
|Event
|PCIe replays were triggered to recover from transmission errors.

|NvidiaPageRetirement
|Event
|The GPU driver has marked a memory page for retirement. This may occur if there is a single double bit error or two single bit errors are encountered at the same address.

|NvidiaPowerError
|Event
|Power utilization of GPUs breached the allowed thresholds.

|NvidiaThermalError
|Event
|Thermal status of GPUs breached the allowed thresholds.

|NvidiaXID[Code]Error
|Condition
|A critical GPU error occurred.

|NvidiaXID[Code]Warning
|Event
|A non-critical GPU error occurred.

|===

[#node-health-ContainerRuntime]
=== ContainerRuntime node health issues

The monitoring condition is `ContainerRuntimeReady` for issues in the following table that have a severity of "Condition".

[%header,cols="3"]
|===

|Name
|Severity
|Description

|ContainerRuntimeFailed
|Event
|The container runtime has failed to create a container, likely related to any reported issues if occurring repeatedly.

|DeprecatedContainerdConfiguration
|Event
|A container image using deprecated image manifest version 2, schema 1 was recently pulled onto the node through `containerd`.

|KubeletFailed
|Event
|The kubelet entered a failed state.

|LivenessProbeFailures
|Event
|A liveness probe failure was detected, potentially indicating application code issues or insufficient timeout values if occurring repeatedly.

|PodStuckTerminating
|Condition
|A Pod is or was stuck terminating for an excessive amount of time, which can be caused by CRI errors preventing pod state progression.

|ReadinessProbeFailures
|Event
|A readiness probe failure was detected, potentially indicating application code issues or insufficient timeout values if occurring repeatedly.

|[Name]RepeatedRestart
|Event
|A systemd unit is restarting frequently.

|ServiceFailedToStart
|Event
|A systemd unit failed to start.

|===

[#node-health-Kernel]
=== Kernel node health issues

The monitoring condition is `KernelReady` for issues in the following table that have a severity of "Condition".

[%header,cols="3"]
|===

|Name
|Severity
|Description

|AppBlocked
|Event
|The task has been blocked for a long period of time from scheduling, usually caused by being blocked on input or output.

|AppCrash
|Event
|An application on the node has crashed.

|ApproachingKernelPidMax
|Event
|The number of processes is approaching the maximum number of PIDs that are available per the current `kernel.pid_max` setting, after which no more processes can be launched.

|ApproachingMaxOpenFiles
|Event
|The number of open files is approaching the maximum number of possible open files given the current kernel settings, after which opening new files will fail.

|ConntrackExceededKernel
|Event
|Connection tracking exceeded the maximum for the kernel and new connections could not be established, which can result in packet loss.

|ExcessiveZombieProcesses
|Event
|Processes which can't be fully reclaimed are accumulating in large numbers, which indicates application issues and may lead to reaching system process limits.

|ForkFailedOutOfPIDs
|Condition
|A fork or exec call has failed due to the system being out of process IDs or memory, which may be caused by zombie processes or physical memory exhaustion.

|KernelBug
|Event
|A kernel bug was detected and reported by the Linux kernel itself, though this may sometimes be caused by nodes with high CPU or memory usage leading to delayed event processing.

|LargeEnvironment
|Event
|The number of environment variables for this process is larger than expected, potentially caused by many services with `enableServiceLinks` set to true, which may cause performance issues.

|RapidCron
|Event
|A cron job is running faster than every five minutes on this node, which may impact performance if the job consumes significant resources.

|SoftLockup
|Event
|The CPU stalled for a given amount of time.

|===

[#node-health-Networking]
=== Networking node health issues

The monitoring condition is `NetworkingReady` for issues in the following table that have a severity of "Condition".

[%header,cols="3"]
|===

|Name
|Severity
|Description

|BandwidthInExceeded
|Event
|Packets have been queued or dropped because the inbound aggregate bandwidth exceeded the maximum for the instance.

|BandwidthOutExceeded
|Event
|Packets have been queued or dropped because the outbound aggregate bandwidth exceeded the maximum for the instance.

|ConntrackExceeded
|Event
|Connection tracking exceeded the maximum for the instance and new connections could not be established, which can result in packet loss.

|EFAErrorMetric
|Event
|EFA driver metrics shows there is an interface with performance degredation.

|IPAMDInconsistentState
|Event
|The state of the IPAMD checkpoint on disk does not reflect the IPs in the container runtime.

|IPAMDNoIPs
|Event
|IPAMD is out of IP addresses.

|IPAMDNotReady
|Condition
|IPAMD fails to connect to the API server.

|IPAMDNotRunning
|Condition
|The Amazon VPC CNI process was not found to be running.

|IPAMDRepeatedlyRestart
|Event
|Multiple restarts in the IPAMD service have occurred.

|InterfaceNotRunning
|Condition
|This interface appears to not be running or there are network issues.

|InterfaceNotUp
|Condition
|This interface appears to not be up or there are network issues.

|KubeProxyNotReady
|Event
|Kube-proxy failed to watch or list resources.

|LinkLocalExceeded
|Event
|Packets were dropped because the PPS of traffic to local proxy services exceeded the network interface maximum.

|MACAddressPolicyMisconfigured
|Event
|The systemd-networkd link configuration has the incorrect `MACAddressPolicy` value.

|MissingDefaultRoutes
|Event
|There are missing default route rules.

|MissingIPRoutes
|Event
|There are missing routes for Pod IPs.

|MissingIPRules
|Event
|There are missing rules for Pod IPs.

|MissingLoopbackInterface
|Condition
|The loopback interface is missing from this instance, causing failure of services depending on local connectivity.

|NetworkSysctl
|Event
|This node's network `sysctl` settings are potentially incorrect.

|PPSExceeded
|Event
|Packets have been queued or dropped because the bidirectional PPS exceeded the maximum for the instance.

|PortConflict
|Event
|If a Pod uses hostPort, it can write `iptables` rules that override the host's already bound ports, potentially preventing API server access to `kubelet`.

|UnexpectedRejectRule
|Event
|An unexpected `REJECT` or `DROP` rule was found in the `iptables`, potentially blocking expected traffic.

|===

[#node-health-Storage]
=== Storage node health issues

The monitoring condition is `StorageReady` for issues in the following table that have a severity of "Condition".

[%header,cols="3"]
|===

|Name
|Severity
|Description

|EBSInstanceIOPSExceeded
|Event
|Maximum IOPS for the instance was exceeded.

|EBSInstanceThroughputExceeded
|Event
|Maximum Throughput for the instance was exceeded.

|EBSVolumeIOPSExceeded
|Event
|Maximum IOPS to a particular EBS Volume was exceeded.

|EBSVolumeThroughputExceeded
|Event
|Maximum Throughput to a particular Amazon EBS volume was exceeded.

|EtcHostsMountFailed
|Event
|Mounting of the kubelet generated `/etc/hosts` failed due to userdata remounting `/var/lib/kubelet/pods` during `kubelet-container` operation.

|IODelays
|Event
|Input or output delay detected in a process, potentially indicating insufficient input-output provisioning if excessive.

|KubeletDiskUsageSlow
|Event
|The `kubelet` is reporting slow disk usage while trying to access the filesystem. This potentially indicates insufficient disk input-output or filesystem issues.

|XFSSmallAverageClusterSize
|Event
|The XFS Average Cluster size is small, indicating excessive free space fragmentation. This can prevent file creation despite available inodes or free space.

|===
